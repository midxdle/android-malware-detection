import numpy as np
from collections import Counter
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_score, recall_score

class DecisionTreeClassifier:
    def __init__(self, max_depth=None):
        self.max_depth = max_depth
        self.tree = None

    def fit(self, X, y):
        self.tree = self._build_tree(X, y, depth=0)

    def _build_tree(self, X, y, depth):
        X = np.array(X)
        y = np.array(y)
        n_samples, n_features = X.shape
        unique_classes = np.unique(y)

        if depth == self.max_depth or len(unique_classes) == 1:
            return Counter(y).most_common(1)[0][0]

        best_gain = -1
        best_feature = None
        for feature in range(n_features):
            values = X[:, feature]
            unique_values = np.unique(values)
            for value in unique_values:
                left_indices = np.where(values <= value)
                right_indices = np.where(values > value)
                left_y = y[left_indices]
                right_y = y[right_indices]
                if len(left_y) > 0 and len(right_y) > 0:
                    gain = self._information_gain(y, left_y, right_y)
                    if gain > best_gain:
                        best_gain = gain
                        best_feature = (feature, value)

        if best_feature is None:
            return Counter(y).most_common(1)[0][0]

        feature, value = best_feature
        left_indices = np.where(X[:, feature] <= value)
        right_indices = np.where(X[:, feature] > value)
        left_subtree = self._build_tree(X[left_indices], y[left_indices], depth + 1)
        right_subtree = self._build_tree(X[right_indices], y[right_indices], depth + 1)

        return (feature, value, left_subtree, right_subtree)

    def _entropy(self, y):
        class_counts = Counter(y)
        probs = [count / len(y) for count in class_counts.values()]
        entropy = -np.sum([p * np.log2(p) for p in probs])
        return entropy

    def _information_gain(self, parent, left, right):
        parent_entropy = self._entropy(parent)
        left_weight = len(left) / len(parent)
        right_weight = len(right) / len(parent)
        weighted_child_entropy = (left_weight * self._entropy(left)) + (right_weight * self._entropy(right))
        information_gain = parent_entropy - weighted_child_entropy
        return information_gain

    def predict(self, X):
        predictions = [self._predict(sample) for sample in X]
        return np.array(predictions)

    def _predict(self, sample):
        node = self.tree
        while isinstance(node, tuple):
            feature, value, left_subtree, right_subtree = node
            if sample[feature] <= value:
                node = left_subtree
            else:
                node = right_subtree
        return node

data = pd.read_csv("amd.csv", low_memory=False)
label_model = LabelEncoder()
data = data.apply(label_model.fit_transform)
X = data.drop(['class'], axis = 1)
y = data['class']
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=23)

tree_classifier = DecisionTreeClassifier(max_depth=3)
tree_classifier.fit(X, y)

x_test = np.array(x_test)
x_train = np.array(x_train)
y_test = np.array(y_test)
y_train = np.array(y_train)
Y_prediction = tree_classifier.predict(x_test)
print('\nACCURACY SCORE = ', accuracy_score(y_test, Y_prediction), '\n')
print('PRECISION SCORE = ', precision_score(y_test, Y_prediction, average = None), '\n')
print('RECALL SCORE = ', recall_score(y_test, Y_prediction, average = None), '\n')
print('CONFUSION MATRIX = \n', confusion_matrix(y_test, Y_prediction), '\n')
print(classification_report(y_test, Y_prediction, zero_division = 1))
